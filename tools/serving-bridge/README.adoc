= TensorFlow Model Export to Serving bridge
Copyright 2020-2021 (c) Cognizant Digital Business, Evolutionary AI. All rights reserved. Issued under the Apache 2.0 license.
ifdef::env-github[]
:imagesdir:
https://raw.githubusercontent.com/cognizantcodehub/LEAF-ManyMinima/main/docs/artwork
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]

ifndef::env-github[]
:imagesdir: ./
endif::[]

:source-highlighter: pygments
:source-language: go

This tool is motivated by a need to promote machine learning models for serving using TFX model serving.

This software component is a part of the LEAF MLOps offering.

For detailed information about this tools role within model serving infrastructure please read the [Production TensorFlow Model Serving Application Note](../../docs/app-note/model-serving.md).

:toc:

== Introduction

The serving bridge is a daemon deployed within Kubernetes for watching S3 blobs which act as indexes to models present within a bucket, and updating a TFX model server configuration file to activate model serving when the S3 index change.

This software component is designed to be deployed as part of an exported model to model serving pipeline that is entirely automatted.

== Packaging

The serving bridge can be obtained as a container image using the quay.io container registry.

```
docker pull quay.io/leafai/studio-serving-bridge:0.11.0
```

== Kubernetes deployment

Using stencil

```
cd tools/serving-bridge
stencil -input deployment.yaml -values Image=quay.io/leafai/studio-serving-bridge | kubectl apply -f -
```

Copyright Â© 2020-2021 Cognizant Digital Business, Evolutionary AI. All rights reserved. Issued under the Apache 2.0 license.
